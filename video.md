*  [![Arxiv](https://img.shields.io/badge/arXiv-2023\.06-red)]() [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)
[[Paper]](https://arxiv.org/abs/2306.02858) [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/ACL-2024-blue)]() [![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star)](https://github.com/mbzuai-oryx/Video-ChatGPT) [Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/abs/2306.05424v2)
[[Paper]](https://arxiv.org/abs/2306.05424v2)[[Github]](https://github.com/mbzuai-oryx/Video-ChatGPT) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR-2024-blue)]() [![Star](https://img.shields.io/github/stars/rese1f.github.io/MovieChat.svg?style=social&label=Star)](https://github.com/rese1f.github.io/MovieChat) [MovieChat: From Dense Token to Sparse Memory for Long Video Understanding](https://arxiv.org/abs/2307.16449)
[[Paper]](https://arxiv.org/abs/2307.16449)[[Github]](https://rese1f.github.io/MovieChat) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR_Highlight-2024-blue)]() [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/Chat-UniVi) [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://arxiv.org/abs/2311.08046)
[[Paper]](https://arxiv.org/abs/2311.08046)[[Github]](https://github.com/PKU-YuanGroup/Chat-UniVi) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&label=Star)](https://github.com/dvlab-research/LLaMA-VID) [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043)
[[Paper]](https://arxiv.org/abs/2311.17043)[[Github]](https://github.com/dvlab-research/LLaMA-VID) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/ziplab/LongVLM.svg?style=social&label=Star)](https://github.com/ziplab/LongVLM) [LongVLM: Efficient Long Video Understanding via Large Language Models](https://arxiv.org/abs/2404.03384)
[[Paper]](https://arxiv.org/abs/2404.03384)[[Github]](https://github.com/ziplab/LongVLM) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.04-red)]() [![Star](https://img.shields.io/github/stars/magic-research/PLLaVA.svg?style=social&label=Star)](https://github.com/magic-research/PLLaVA) [PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning](https://arxiv.org/abs/2404.16994)
[[Paper]](https://arxiv.org/abs/2404.16994)[[Github]](https://github.com/magic-research/PLLaVA) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.06-red)]() [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/VideoLLaMA2) [VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/abs/2406.07476)
[[Paper]](https://arxiv.org/abs/2406.07476)[[Github]](https://github.com/DAMO-NLP-SG/VideoLLaMA2) [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.07-red)]() [![Star](https://img.shields.io/github/stars/apple/ml-slowfast-llava.svg?style=social&label=Star)](https://github.com/apple/ml-slowfast-llava) [SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models](https://arxiv.org/abs/2407.15841)
[[Paper]](https://arxiv.org/abs/2407.15841)[[Github]](https://github.com/apple/ml-slowfast-llava) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.08-red)]() [![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)](https://github.com/LLaVA-VL/LLaVA-NeXT) [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)
[[Paper]](https://arxiv.org/abs/2408.03326)[[Github]](https://github.com/LLaVA-VL/LLaVA-NeXT) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.09-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2-VL.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen2-VL) [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)
[[Paper]](https://arxiv.org/abs/2409.12191)[[Github]](https://github.com/QwenLM/Qwen2-VL) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.10-red)]() [![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)](https://github.com/LLaVA-VL/LLaVA-NeXT) [Video Instruction Tuning with Synthetic Data](http://arxiv.org/abs/2410.02713)
[[Paper]](http://arxiv.org/abs/2410.02713)[[Github]](https://github.com/LLaVA-VL/LLaVA-NeXT) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&label=Star)](https://github.com/Vision-CAIR/LongVU) [LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/abs/2410.17434)
[[Paper]](https://arxiv.org/abs/2410.17434)[[Github]](https://github.com/Vision-CAIR/LongVU) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR_Highlight-2025-blue)]() [AdaCM2: Adaptive Crossâ€‘Modality Memory Reduction](https://arxiv.org/abs/2411.12593)
[[Paper]](https://arxiv.org/abs/2411.12593) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/KD-TAO/DyCoke?tab=readme-ov-file.svg?style=social&label=Star)](https://github.com/KD-TAO/DyCoke?tab=readme-ov-file) [DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models](https://arxiv.org/abs/2411.15024)
[[Paper]](https://arxiv.org/abs/2411.15024)[[Github]](https://github.com/KD-TAO/DyCoke?tab=readme-ov-file) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Hon-Wong/ByteVideoLLM.svg?style=social&label=Star)](https://github.com/Hon-Wong/ByteVideoLLM) [Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/abs/2412.09530)
[[Paper]](https://arxiv.org/abs/2412.09530)[[Github]](https://github.com/Hon-Wong/ByteVideoLLM) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/PVC.svg?style=social&label=Star)](https://github.com/OpenGVLab/PVC) [PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models](https://arxiv.org/abs/2412.09613)
[[Paper]](https://arxiv.org/abs/2412.09613)[[Github]](https://github.com/OpenGVLab/PVC) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/Visual-AI/PruneVid.svg?style=social&label=Star)](https://github.com/Visual-AI/PruneVid) [PruneVid: Visual Token Pruning for Efficient Video Large Language Models](https://arxiv.org/abs/2412.16117v1)
[[Paper]](https://arxiv.org/abs/2412.16117v1)[[Github]](https://github.com/Visual-AI/PruneVid) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention_Based-green)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/thu-nics/FrameFusion.svg?style=social&label=Star)](https://github.com/thu-nics/FrameFusion) [FrameFusion: Combining Similarity and Importance for Video Token Reduction
on Large Visual Language Models](https://arxiv.org/abs/2501.01986)
[[Paper]](https://arxiv.org/abs/2501.01986)[[Github]](https://github.com/thu-nics/FrameFusion) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention_Based-green)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]()
*  [![Publish](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/ictnlp/LLaVA-Mini.svg?style=social&label=Star)](https://github.com/ictnlp/LLaVA-Mini) [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/abs/2501.03895)
[[Paper]](https://arxiv.org/abs/2501.03895)[[Github]](https://github.com/ictnlp/LLaVA-Mini) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen2.5-VL) [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)
[[Paper]](https://arxiv.org/abs/2502.13923)[[Github]](https://github.com/QwenLM/Qwen2.5-VL) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Publish](https://img.shields.io/badge/NAACL-2025-blue)]() [MEDA: Dynamic KV Cache Allocation for Efficient
Multimodal Long-Context Inference](https://arxiv.org/abs/2502.17599)
[[Paper]](https://arxiv.org/abs/2502.17599) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/abs/2503.04130v1)
[[Paper]](https://arxiv.org/abs/2503.04130v1) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](https://arxiv.org/abs/2503.10742v1)
[[Paper]](https://arxiv.org/abs/2503.10742v1) [![Area](https://img.shields.io/badge/Video_LLM-purple)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/LunarShen/FastVID.svg?style=social&label=Star)](https://github.com/LunarShen/FastVID) [FastVID: Dynamic Density Pruning for Fast Video Large Language Models](https://arxiv.org/abs/2503.11187)
[[Paper]](https://arxiv.org/abs/2503.11187)[[Github]](https://github.com/LunarShen/FastVID) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention_Based-green)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/ludc506/InternVL-X.svg?style=social&label=Star)](https://github.com/ludc506/InternVL-X) [InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](https://arxiv.org/abs/2503.21307)
[[Paper]](https://arxiv.org/abs/2503.21307)[[Github]](https://github.com/ludc506/InternVL-X) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.04-red)]() [QG-VTC: Question-Guided Visual Token Compression in MLLMs for Efficient VQA](https://arxiv.org/abs/2504.00654)
[[Paper]](https://arxiv.org/abs/2504.00654) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.04-red)]() [![Star](https://img.shields.io/github/stars/yaolinli/TimeChat-Online.svg?style=social&label=Star)](https://github.com/yaolinli/TimeChat-Online) [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)
[[Paper]](https://arxiv.org/abs/2504.17343)[[Github]](https://github.com/yaolinli/TimeChat-Online) [![Area](https://img.shields.io/badge/Streaming_Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ByteDance-Seed/Seed1.5-VL.svg?style=social&label=Star)](https://github.com/ByteDance-Seed/Seed1.5-VL) [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062)
[[Paper]](https://arxiv.org/abs/2505.07062)[[Github]](https://github.com/ByteDance-Seed/Seed1.5-VL) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/VidCom2.svg?style=social&label=Star)](https://github.com/xuyang-liu16/VidCom2) [Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models](https://arxiv.org/abs/2505.14454)
[[Paper]](https://arxiv.org/abs/2505.14454)[[Github]](https://github.com/xuyang-liu16/VidCom2) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/shilinyan99/CrossLMM.svg?style=social&label=Star)](https://github.com/shilinyan99/CrossLMM) [CrossLMM: Decoupling Long Video Sequences from
LMMs via Dual Cross-Attention Mechanisms](https://arxiv.org/abs/2505.17020)
[[Paper]](https://arxiv.org/abs/2505.17020)[[Github]](https://github.com/shilinyan99/CrossLMM) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]() [![Cost](https://img.shields.io/badge/Need_Training-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [AdaTP: Attention-Debiased Token Pruning for
Video Large Language Models](https://arxiv.org/abs/2505.20100)
[[Paper]](https://arxiv.org/abs/2505.20100) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention_Based-green)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/HoliTom.svg?style=social&label=Star)](https://github.com/cokeshao/HoliTom) [HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/abs/2505.21334)
[[Paper]](https://arxiv.org/abs/2505.21334)[[Github]](https://github.com/cokeshao/HoliTom) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention_Based-green)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/yunzhuzhang0918/flexselect.svg?style=social&label=Star)](https://github.com/yunzhuzhang0918/flexselect) [FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](https://arxiv.org/abs/2506.00993)
[[Paper]](https://arxiv.org/abs/2506.00993)[[Github]](https://github.com/yunzhuzhang0918/flexselect) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention_Based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/mnyuew/METok.svg?style=social&label=Star)](https://github.com/mnyuew/METok) [METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding](https://arxiv.org/abs/2506.02850)
[[Paper]](https://arxiv.org/abs/2506.02850)[[Github]](https://github.com/mnyuew/METok) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention_Based-green)]() [![Type](https://img.shields.io/badge/Query_Based-green)]() [![Type](https://img.shields.io/badge/Transformation_Based-green)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
[[Paper]](https://arxiv.org/abs/2506.03990) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/HumanMLLM/LLaVA-Scissor.svg?style=social&label=Star)](https://github.com/HumanMLLM/LLaVA-Scissor) [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://www.arxiv.org/abs/2506.21862)
[[Paper]](https://www.arxiv.org/abs/2506.21862)[[Github]](https://github.com/HumanMLLM/LLaVA-Scissor) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]()
*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/HYUNJS/STTM.svg?style=social&label=Star)](https://github.com/HYUNJS/STTM) [Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs](https://arxiv.org/abs/2507.07990)
[[Paper]](https://arxiv.org/abs/2507.07990)[[Github]](https://github.com/HYUNJS/STTM) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity_based-green)]() [![Cost](https://img.shields.io/badge/Training_free-yellow)]()
